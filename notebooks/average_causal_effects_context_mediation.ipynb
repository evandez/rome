{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f379178d",
   "metadata": {},
   "source": [
    "# Script to plot average causal effects\n",
    "\n",
    "This script loads sets of hundreds of causal traces that have been computed by the\n",
    "`experiment.causal_trace` program, and then aggregates the results to compute\n",
    "Average Indirect Effects and Average Total Effects as well as some other information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bba71c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy, os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"/raid/lingo/dez/code/rome\")\n",
    "from experiments import causal_trace\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "\n",
    "# Uncomment the architecture to plot.\n",
    "# arch = \"gpt2-xl\"\n",
    "# archname = \"GPT-2-XL\"\n",
    "\n",
    "# arch = 'EleutherAI_gpt-j-6B'\n",
    "# archname = 'GPT-J-6B'\n",
    "\n",
    "# arch = 'EleutherAI_gpt-neox-20b'\n",
    "# archname = 'GPT-NeoX-20B'\n",
    "\n",
    "\n",
    "class Avg:\n",
    "    def __init__(self):\n",
    "        self.d = []\n",
    "\n",
    "    def add(self, v):\n",
    "        self.d.append(v[None])\n",
    "\n",
    "    def add_all(self, vv):\n",
    "        self.d.append(vv)\n",
    "\n",
    "    def avg(self):\n",
    "        return numpy.concatenate(self.d).mean(axis=0)\n",
    "\n",
    "    def std(self):\n",
    "        return numpy.concatenate(self.d).std(axis=0)\n",
    "\n",
    "    def size(self):\n",
    "        return sum(datum.shape[0] for datum in self.d)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_tokenizer(arch=\"gpt2\"):\n",
    "    import transformers\n",
    "    return transformers.AutoTokenizer.from_pretrained(arch)\n",
    "\n",
    "def get_raws(dataset=\"winoventi\", mediation=\"med\", corrupted=\"subj_last\"):\n",
    "    file_full_subj = Path(f\"../data/mediation/full/{dataset}_med_subj_last.json\")\n",
    "    with file_full_subj.open(\"r\") as handle:\n",
    "        raws_full_subj = json.load(handle)\n",
    "\n",
    "    file_full_attr = Path(f\"../data/mediation/full/{dataset}_med_attr.json\")\n",
    "    with file_full_attr.open(\"r\") as handle:\n",
    "        raws_full_attr = json.load(handle)\n",
    "\n",
    "    raws_full_subj_by_prompt = {raw[\"prompt\"]: raw for raw in raws_full_subj}\n",
    "    raws_full_attr_by_prompt = {raw[\"prompt\"]: raw for raw in raws_full_attr}\n",
    "    for prompt, raw in raws_full_subj_by_prompt.items():\n",
    "        raw[\"attribute\"] = raws_full_attr_by_prompt[prompt][\"subject\"]\n",
    "    \n",
    "    # Now load subset used during causal tracing.\n",
    "    file_ct = Path(f\"../data/mediation/{dataset}_{mediation}_{corrupted}.json\")\n",
    "    with file_ct.open(\"r\") as handle:\n",
    "        raws = json.load(handle)\n",
    "    \n",
    "    raws_by_known_id = {\n",
    "        raw[\"known_id\"]: raws_full_subj_by_prompt[raw[\"prompt\"]]\n",
    "        for raw in raws\n",
    "    }\n",
    "    return raws_by_known_id\n",
    "\n",
    "\n",
    "def read_knowlege(kind=None,\n",
    "                  arch=\"gpt2\",\n",
    "                  dataset=\"winoventi\",\n",
    "                  mediation=\"med\",\n",
    "                  corrupted=\"subj_first\"):\n",
    "    dirname = Path(f\"../results/ns3_r0_{arch}/{dataset}_{mediation}_{corrupted}/causal_trace/cases\")\n",
    "    kindcode = \"\" if not kind else f\"_{kind}\"\n",
    "    pattern = re.compile(f\"knowledge_(\\d+){kindcode}.npz\")\n",
    "    tokenizer = get_tokenizer(arch.lower())\n",
    "    size = 0\n",
    "    raws_by_known_id = get_raws(dataset=dataset, mediation=mediation, corrupted=corrupted)\n",
    "    (\n",
    "        avg_subj_first,\n",
    "        avg_after_subj_first,\n",
    "        avg_attr,\n",
    "        avg_after_attr,\n",
    "        avg_subj_last,\n",
    "        avg_after_subj_last,\n",
    "        avg_ls,\n",
    "    ) = [Avg() for _ in range(7)]\n",
    "    for file in tqdm(Path(dirname).glob(f\"knowledge_*{kindcode}.npz\"), desc=f\"kind={kind}\"):\n",
    "        try:\n",
    "            data = numpy.load(str(file))\n",
    "        except:\n",
    "            continue\n",
    "        # Only consider cases where the model begins with the correct prediction\n",
    "        if \"correct_prediction\" in data and not data[\"correct_prediction\"]:\n",
    "            continue\n",
    "\n",
    "        # Parse known ID, if it fails it means we're in case kindcode=\"\" and file is\n",
    "        # kind mlp/attn.\n",
    "        match = pattern.match(file.name)\n",
    "        if match is None:\n",
    "            continue\n",
    "        known_id = int(match.group(1))\n",
    "\n",
    "        raw = raws_by_known_id[known_id]\n",
    "        tokens_prompt = tokenizer(raw[\"prompt\"], add_special_tokens=False).input_ids\n",
    "        try:\n",
    "            subj_first_i, subj_first_j = causal_trace.find_token_range(tokenizer, tokens_prompt, raw[\"subject\"])\n",
    "            subj_last_i, subj_last_j = causal_trace.find_token_range(tokenizer, tokens_prompt, raw[\"subject\"],\n",
    "                                                                     occurrence=raw[\"occurrence\"])\n",
    "            attr_i, attr_j = causal_trace.find_token_range(tokenizer, tokens_prompt, raw[\"attribute\"])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "        size += 1\n",
    "        scores = data[\"scores\"]\n",
    "        avg_ls.add(data[\"low_score\"])\n",
    "        avg_subj_first.add_all(scores[subj_first_i:subj_first_j])\n",
    "        avg_after_subj_first.add_all(scores[subj_first_j:attr_i])\n",
    "        avg_attr.add_all(scores[attr_i:attr_j])\n",
    "        avg_after_attr.add_all(scores[attr_j:subj_last_i])\n",
    "        avg_subj_last.add_all(scores[subj_last_i:subj_last_j])\n",
    "        avg_after_subj_last.add_all(scores[subj_last_j:])\n",
    "\n",
    "    result = numpy.stack(\n",
    "        [\n",
    "            avg_subj_first.avg(),\n",
    "            avg_after_subj_first.avg(),\n",
    "            avg_attr.avg(),\n",
    "            avg_after_attr.avg(),\n",
    "            avg_subj_last.avg(),\n",
    "            avg_after_subj_last.avg(),\n",
    "        ]\n",
    "    )\n",
    "    result_std = numpy.stack(\n",
    "        [\n",
    "            avg_subj_first.std(),\n",
    "            avg_after_subj_first.std(),\n",
    "            avg_attr.std(),\n",
    "            avg_after_attr.std(),\n",
    "            avg_subj_last.std(),\n",
    "            avg_after_subj_last.std(),\n",
    "        ]\n",
    "    )\n",
    "    return dict(\n",
    "        low_score=avg_ls.avg(), result=result, result_std=result_std, size=size\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_array(\n",
    "    differences,\n",
    "    kind=None,\n",
    "    savepdf=None,\n",
    "    title=None,\n",
    "    low_score=None,\n",
    "    high_score=None,\n",
    "    archname=\"GPT2\",\n",
    "    corrupted=\"subj_first\",\n",
    "):\n",
    "    if low_score is None:\n",
    "        low_score = differences.min()\n",
    "    if high_score is None:\n",
    "        high_score = differences.max()\n",
    "    answer = \"AIE\"\n",
    "    labels = [\n",
    "        \"First subj mention\" + (\"*\" if corrupted == \"subj_first\" else \"\"),\n",
    "        \"Between subj and attr\",\n",
    "        \"Attr\" + (\"*\" if corrupted == \"attr\" else \"\"),\n",
    "        \"Between attr and subj\",\n",
    "        \"Second subj mention\" + (\"*\" if corrupted == \"subj_last\" else \"\"),\n",
    "        \"Last tokens\",\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), dpi=200)\n",
    "    h = ax.pcolor(\n",
    "        differences,\n",
    "        cmap={None: \"Purples\", \"mlp\": \"Greens\", \"attn\": \"Reds\"}[kind],\n",
    "        vmin=low_score,\n",
    "        vmax=high_score,\n",
    "    )\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks([0.5 + i for i in range(len(differences))])\n",
    "    ax.set_xticks([0.5 + i for i in range(0, differences.shape[1] - 6, 5)])\n",
    "    ax.set_xticklabels(list(range(0, differences.shape[1] - 6, 5)))\n",
    "    ax.set_yticklabels(labels)\n",
    "    if kind is None:\n",
    "        ax.set_xlabel(f\"single patched layer within {archname}\")\n",
    "    else:\n",
    "        ax.set_xlabel(f\"center of interval of patched {kind} layers\")\n",
    "    cb = plt.colorbar(h)\n",
    "    # The following should be cb.ax.set_xlabel(answer), but this is broken in matplotlib 3.5.1.\n",
    "    if answer:\n",
    "        cb.ax.set_title(str(answer).strip(), y=-0.16, fontsize=10)\n",
    "\n",
    "    if savepdf:\n",
    "        os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
    "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "arch = \"gpt2\"\n",
    "archname = \"GPT-2\"\n",
    "dataset = \"CounterFact\"\n",
    "mediation = \"med\"\n",
    "corrupted = \"attr\"\n",
    "high_score = None  # Scale all plots according to the y axis of the first plot\n",
    "\n",
    "for kind in [None, \"mlp\", \"attn\"]:\n",
    "    d = read_knowlege(kind=kind,\n",
    "                      arch=arch,\n",
    "                      dataset=dataset.lower(),\n",
    "                      mediation=mediation,\n",
    "                      corrupted=corrupted)\n",
    "    count = d[\"size\"]\n",
    "    what = {\n",
    "        None: \"Indirect Effect of $h_i^{(l)}$\",\n",
    "        \"mlp\": \"Indirect Effect of MLP\",\n",
    "        \"attn\": \"Indirect Effect of Attn\",\n",
    "    }[kind]\n",
    "    title = f\"[{dataset}/{'Med' if mediation == 'med' else 'Unmed'}] Avg {what} over {count} prompts\"\n",
    "    result = numpy.clip(d[\"result\"] - d[\"low_score\"], 0, None)\n",
    "    kindcode = \"\" if kind is None else f\"_{kind}\"\n",
    "    if kind not in [\"mlp\", \"attn\"]:\n",
    "        high_score = result.max()\n",
    "    plot_array(\n",
    "        result,\n",
    "        kind=kind,\n",
    "        title=title,\n",
    "        low_score=0.0,\n",
    "        high_score=high_score,\n",
    "        archname=archname,\n",
    "        corrupted=corrupted,\n",
    "        savepdf=f\"results/{arch}/causal_trace/summary_pdfs/rollup{kindcode}.pdf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896e9ac",
   "metadata": {},
   "source": [
    "## Plot line graph\n",
    "\n",
    "To make confidence intervals visible, we plot the data as line graphs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "labels = [\n",
    "    \"First subject token\",\n",
    "    \"Middle subject tokens\",\n",
    "    \"Last subject token\",\n",
    "    \"First subsequent token\",\n",
    "    \"Further tokens\",\n",
    "    \"Last token\",\n",
    "]\n",
    "color_order = [0, 1, 2, 4, 5, 3]\n",
    "x = None\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 3.5), sharey=True, dpi=200)\n",
    "for j, (kind, title) in enumerate(\n",
    "    [\n",
    "        (None, \"single hidden vector\"),\n",
    "        (\"mlp\", \"run of 10 MLP lookups\"),\n",
    "        (\"attn\", \"run of 10 Attn modules\"),\n",
    "    ]\n",
    "):\n",
    "    print(f\"Reading {kind}\")\n",
    "    d = read_knowlege(225, kind, arch)\n",
    "    for i, label in list(enumerate(labels)):\n",
    "        y = d[\"result\"][i] - d[\"low_score\"]\n",
    "        if x is None:\n",
    "            x = list(range(len(y)))\n",
    "        std = d[\"result_std\"][i]\n",
    "        error = std * 1.96 / math.sqrt(count)\n",
    "        axes[j].fill_between(\n",
    "            x, y - error, y + error, alpha=0.3, color=cmap.colors[color_order[i]]\n",
    "        )\n",
    "        axes[j].plot(x, y, label=label, color=cmap.colors[color_order[i]])\n",
    "\n",
    "    axes[j].set_title(f\"Average indirect effect of a {title}\")\n",
    "    axes[j].set_ylabel(\"Average indirect effect on p(o)\")\n",
    "    axes[j].set_xlabel(f\"Layer number in {archname}\")\n",
    "    # axes[j].set_ylim(0.1, 0.3)\n",
    "axes[1].legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"results/{arch}/causal_trace/summary_pdfs/lineplot-causaltrace.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
